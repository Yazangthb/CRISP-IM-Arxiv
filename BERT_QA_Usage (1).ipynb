{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"gpuType":"T4","authorship_tag":"ABX9TyPmhzmroniKWRwTpAL7ssd7"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"code","source":["# !pip install torch transformers"],"metadata":{"id":"4SauCZbWf902"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import torch\n","from transformers import BertForQuestionAnswering\n","from transformers import BertTokenizer\n","import os"],"metadata":{"id":"eoEyKPIaf4XO"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import requests\n","import json\n","name = 'tokenizer.json'\n","url = f'https://huggingface.co/bert-large-uncased-whole-word-masking-finetuned-squad/raw/main/{name}'\n","resp = requests.get(url, verify=False)\n","dict_ = resp.json()\n","json_object = json.dumps(dict_, indent = 4)\n","\n","\n","url = 'https://huggingface.co/bert-large-uncased-whole-word-masking-finetuned-squad/raw/main/vocab.txt'\n","resp = requests.get(url,verify=False)\n","dict_ = resp.text\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"CKdntBfYgDQJ","executionInfo":{"status":"ok","timestamp":1742389442522,"user_tz":-180,"elapsed":883,"user":{"displayName":"yazan nukari","userId":"00359427674085351183"}},"outputId":"522baf57-8e59-465e-d3fa-22a2d92f4eaf"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.11/dist-packages/urllib3/connectionpool.py:1097: InsecureRequestWarning: Unverified HTTPS request is being made to host 'huggingface.co'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/latest/advanced-usage.html#tls-warnings\n","  warnings.warn(\n","/usr/local/lib/python3.11/dist-packages/urllib3/connectionpool.py:1097: InsecureRequestWarning: Unverified HTTPS request is being made to host 'huggingface.co'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/latest/advanced-usage.html#tls-warnings\n","  warnings.warn(\n","/usr/local/lib/python3.11/dist-packages/urllib3/connectionpool.py:1097: InsecureRequestWarning: Unverified HTTPS request is being made to host 'huggingface.co'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/latest/advanced-usage.html#tls-warnings\n","  warnings.warn(\n","/usr/local/lib/python3.11/dist-packages/urllib3/connectionpool.py:1097: InsecureRequestWarning: Unverified HTTPS request is being made to host 'huggingface.co'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/latest/advanced-usage.html#tls-warnings\n","  warnings.warn(\n"]}]},{"cell_type":"code","source":["import torch\n","from transformers import BertForQuestionAnswering, BertTokenizer\n","\n","def find_answer_text(question, text):\n","    model_name = 'bert-large-uncased-whole-word-masking-finetuned-squad'\n","    model = BertForQuestionAnswering.from_pretrained(model_name)\n","    tokenizer = BertTokenizer.from_pretrained(model_name)\n","\n","    input_ids = tokenizer.encode(question, text)\n","    tokens = tokenizer.convert_ids_to_tokens(input_ids)\n","\n","    # Locate the first [SEP] token\n","    sep_idx = input_ids.index(tokenizer.sep_token_id)\n","    num_seg_a = sep_idx + 1  # Tokens in segment A (question)\n","    num_seg_b = len(input_ids) - num_seg_a  # Tokens in segment B (text)\n","    segment_ids = [0] * num_seg_a + [1] * num_seg_b  # Segment identifiers\n","\n","    assert len(segment_ids) == len(input_ids)\n","\n","    # Model inference\n","    output = model(\n","        torch.tensor([input_ids]),\n","        token_type_ids=torch.tensor([segment_ids])\n","    )\n","\n","    # Extract answer\n","    answer_start = torch.argmax(output.start_logits)\n","    answer_end = torch.argmax(output.end_logits)\n","\n","    if answer_end >= answer_start:\n","        answer = \" \".join(tokens[answer_start:answer_end + 1])\n","    else:\n","        return \"I am unable to find the answer to this question. Can you please ask another question?\"\n","\n","    formatted_answer = answer.capitalize()\n","    print(f\"\\nQuestion:\\n{question.capitalize()}\")\n","    print(f\"\\nAnswer:\\n{formatted_answer}.\")\n","\n","    return formatted_answer"],"metadata":{"id":"jXXrYgdkhEIB"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["!pip install pdfplumber"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"axctz02vonEi","executionInfo":{"status":"ok","timestamp":1742391647210,"user_tz":-180,"elapsed":4842,"user":{"displayName":"yazan nukari","userId":"00359427674085351183"}},"outputId":"a44b1fce-ee9e-48e1-d0b9-fb35405ddd93"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting pdfplumber\n","  Downloading pdfplumber-0.11.5-py3-none-any.whl.metadata (42 kB)\n","\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/42.5 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.5/42.5 kB\u001b[0m \u001b[31m2.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting pdfminer.six==20231228 (from pdfplumber)\n","  Downloading pdfminer.six-20231228-py3-none-any.whl.metadata (4.2 kB)\n","Requirement already satisfied: Pillow>=9.1 in /usr/local/lib/python3.11/dist-packages (from pdfplumber) (11.1.0)\n","Collecting pypdfium2>=4.18.0 (from pdfplumber)\n","  Downloading pypdfium2-4.30.1-py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (48 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m48.2/48.2 kB\u001b[0m \u001b[31m3.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: charset-normalizer>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from pdfminer.six==20231228->pdfplumber) (3.4.1)\n","Requirement already satisfied: cryptography>=36.0.0 in /usr/local/lib/python3.11/dist-packages (from pdfminer.six==20231228->pdfplumber) (43.0.3)\n","Requirement already satisfied: cffi>=1.12 in /usr/local/lib/python3.11/dist-packages (from cryptography>=36.0.0->pdfminer.six==20231228->pdfplumber) (1.17.1)\n","Requirement already satisfied: pycparser in /usr/local/lib/python3.11/dist-packages (from cffi>=1.12->cryptography>=36.0.0->pdfminer.six==20231228->pdfplumber) (2.22)\n","Downloading pdfplumber-0.11.5-py3-none-any.whl (59 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m59.5/59.5 kB\u001b[0m \u001b[31m4.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading pdfminer.six-20231228-py3-none-any.whl (5.6 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.6/5.6 MB\u001b[0m \u001b[31m75.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading pypdfium2-4.30.1-py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.9 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.9/2.9 MB\u001b[0m \u001b[31m79.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hInstalling collected packages: pypdfium2, pdfminer.six, pdfplumber\n","Successfully installed pdfminer.six-20231228 pdfplumber-0.11.5 pypdfium2-4.30.1\n"]}]},{"cell_type":"code","source":["# import pdfplumber\n","\n","# pdf_path = \"paper.pdf\"\n","\n","# full_text = \"\"\n","# with pdfplumber.open(pdf_path) as pdf:a\n","#     for page in pdf.pages:\n","#         text = page.extract_text()\n","#         if text:  # Avoid None values\n","#             full_text += text + \"\\n\"\n","\n","# print(full_text)  # Print or process the extracted text\n"],"metadata":{"id":"vWG9vr_mofXe"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["question = 'What are the problems the authors are solving?'\n","question = 'What solution is presented?'\n","# question = 'What are the limitations of the solution?'\n","text = '''KOMPSAT-3 is a high performance remote sensing satellite, which provides 0.7 m GSD\n","panchromatic image and 2.8 m GSD multi-spectral image data for various applications.\n","KOMPSAT-3 was launched into a sun synchronous low Earth orbit on the 18th of May, 2012\n","and the life time of more than 7 years is expected.'''\n","text2 = '''Forecasting high-impact research topics\n","via machine learning on evolving knowledge graphs\n","Xuemei Gu1, ∗ and Mario Krenn1, †\n","1Max Planck Institute for the Science of Light, Staudtstrasse 2, 91058 Erlangen, Germany\n","The exponential growth in scientific publications poses a severe challenge for human researchers.\n","It forces attention to more narrow sub-fields, which makes it challenging to discover new impactful\n","research ideas and collaborations outside one’s own field. While there are ways to predict a scientific\n","paper’s future citation counts, they need the research to be finished and the paper written, usually\n","assessing impact long after the idea was conceived. Here we show how to predict the impact of\n","onsets of ideas that have never been published by researchers. For that, we developed a large\n","evolving knowledge graph built from more than 21 million scientific papers. It combines a semantic\n","network created from the content of the papers and an impact network created from the historic\n","citations of papers. Using machine learning, we can predict the dynamic of the evolving network\n","into the future with high accuracy (AUC values beyond 0.9 for most experiments), and thereby the\n","impact of new research directions. We envision that the ability to predict the impact of new ideas\n","will be a crucial component of future artificial muses that can inspire new impactful and interesting\n","scientific ideas.\n","'''\n","\n","answer = find_answer_text(question,text2)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"naUzn5y1gSbg","executionInfo":{"status":"ok","timestamp":1742391858025,"user_tz":-180,"elapsed":3705,"user":{"displayName":"yazan nukari","userId":"00359427674085351183"}},"outputId":"8dd60fe9-bed6-440c-b55c-b0d0e33099e3"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["Some weights of the model checkpoint at bert-large-uncased-whole-word-masking-finetuned-squad were not used when initializing BertForQuestionAnswering: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']\n","- This IS expected if you are initializing BertForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing BertForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"]},{"output_type":"stream","name":"stdout","text":["\n","Question:\n","What solution is presented?\n","\n","Answer:\n","Using machine learning , we can predict the dynamic of the evolving network into the future with high accuracy ( au ##c values beyond 0 . 9 for most experiments ) , and thereby the impact of new research directions.\n"]}]},{"cell_type":"code","source":["from transformers import AutoTokenizer, AutoModelForCausalLM\n","\n","# Replace with the actual model name (it should be a Qwen model with < 3B parameters)\n","model_name = \"Qwen/Qwen2.5-0.5B\"\n","\n","# Load the tokenizer and model\n","tokenizer = AutoTokenizer.from_pretrained(model_name)\n","# Load the model and move it to GPU if available\n","model = AutoModelForCausalLM.from_pretrained(model_name).to(\"cuda\")\n","\n","\n","def extract_info(abstract: str) -> dict:\n","    # Construct a prompt that instructs the model on what to extract\n","    prompt = (\n","        f'''\n","        Extract from the article text the following information:\\n\n","        - Problem:\\n\n","        - Solution:\\n\n","        - Limitation:\\n\n","        return a concise answer with the same format as the prompt.\n","        article text: {abstract}\\n\\n\n","        '''\n","    )\n","\n","    # Encode prompt and generate output\n","    inputs = {k: v.to(\"cuda\") for k, v in tokenizer(prompt, return_tensors=\"pt\").items()}\n","\n","    outputs = model.generate(**inputs, max_length=512, num_return_sequences=1)\n","    generated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n","    print(generated_text)\n","    # Initialize the result dictionary\n","    result = {\"problem\": None, \"solution\": None, \"limitation\": None}\n","\n","    # Parse the output to extract each section by looking for markers\n","    for line in generated_text.splitlines():\n","        line = line.strip()\n","        if line.lower().startswith(\"problem:\"):\n","            result[\"problem\"] = line[len(\"problem:\"):].strip()\n","        elif line.lower().startswith(\"solution:\"):\n","            result[\"solution\"] = line[len(\"solution:\"):].strip()\n","        elif line.lower().startswith(\"limitation:\"):\n","            result[\"limitation\"] = line[len(\"limitation:\"):].strip()\n","\n","    return result\n","\n","# Example usage:\n","if __name__ == \"__main__\":\n","    abstract_text = (\n","        \"In this paper, we address the challenge of efficiently training neural networks on limited data. \"\n","        \"Our solution introduces a novel data augmentation technique combined with transfer learning. \"\n","        \"However, our approach is limited by the increased computational overhead in the augmentation step.\"\n","    )\n","\n","    info = extract_info(abstract_text)\n","    print(\"Extracted Information:\")\n","    print(\"Problem    :\", info[\"problem\"])\n","    print(\"Solution   :\", info[\"solution\"])\n","    print(\"Limitation :\", info[\"limitation\"])\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Ycpuo5a6glVH","executionInfo":{"status":"ok","timestamp":1742391376091,"user_tz":-180,"elapsed":66865,"user":{"displayName":"yazan nukari","userId":"00359427674085351183"}},"outputId":"4decec81-2a8a-4a5b-cb88-a90132451c32"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["Setting `pad_token_id` to `eos_token_id`:151643 for open-end generation.\n","Both `max_new_tokens` (=2048) and `max_length`(=512) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"]},{"output_type":"stream","name":"stdout","text":["\n","        Extract from the article text the following information:\n","\n","        - Problem:\n","\n","        - Solution:\n","\n","        - Limitation:\n","\n","        return a concise answer with the same format as the prompt.\n","        article text: In this paper, we address the challenge of efficiently training neural networks on limited data. Our solution introduces a novel data augmentation technique combined with transfer learning. However, our approach is limited by the increased computational overhead in the augmentation step.\n","\n","\n","        \n","\n","\n","        \"\"\"\n","        # Write your code here\n","        # Hint: You can use the following code to get the number of words in the article\n","        # words = re.findall(r'\\w+', article_text)\n","        # print(len(words))\n","        # Hint: You can use the following code to get the number of sentences in the article\n","        # sentences = re.findall(r'(?<=\\s)\\w+', article_text)\n","        # print(len(sentences))\n","        # Hint: You can use the following code to get the number of paragraphs in the article\n","        # paragraphs = re.findall(r'(?<=\\n\\s)\\w+', article_text)\n","        # print(len(paragraphs))\n","        # Hint: You can use the following code to get the number of sentences in the article\n","        # sentences = re.findall(r'(?<=\\s)\\w+', article_text)\n","        # print(len(sentences))\n","        # Hint: You can use the following code to get the number of paragraphs in the article\n","        # paragraphs = re.findall(r'(?<=\\n\\s)\\w+', article_text)\n","        # print(len(paragraphs))\n","        # Hint: You can use the following code to get the number of sentences in the article\n","        # sentences = re.findall(r'(?<=\\s)\\w+', article_text)\n","        # print(len(sentences))\n","        # Hint: You can use the following code to get the number of paragraphs in the article\n","        # paragraphs = re.findall(r'(?<=\\n\\s)\\w+', article_text)\n","        # print(len(paragraphs))\n","        # Hint: You can use the following code to get the number of sentences in the article\n","        # sentences = re.findall(r'(?<=\\s)\\w+', article_text)\n","        # print(len(sentences))\n","        # Hint: You can use the following code to get the number of paragraphs in the article\n","        # paragraphs = re.findall(r'(?<=\\n\\s)\\w+', article_text)\n","        # print(len(paragraphs))\n","        # Hint: You can use the following code to get the number of sentences in the article\n","        # sentences = re.findall(r'(?<=\\s)\\w+', article_text)\n","        # print(len(sentences))\n","        # Hint: You can use the following code to get the number of paragraphs in the article\n","        # paragraphs = re.findall(r'(?<=\\n\\s)\\w+', article_text)\n","        # print(len(paragraphs))\n","        # Hint: You can use the following code to get the number of sentences in the article\n","        # sentences = re.findall(r'(?<=\\s)\\w+', article_text)\n","        # print(len(sentences))\n","        # Hint: You can use the following code to get the number of paragraphs in the article\n","        # paragraphs = re.findall(r'(?<=\\n\\s)\\w+', article_text)\n","        # print(len(paragraphs))\n","        # Hint: You can use the following code to get the number of sentences in the article\n","        # sentences = re.findall(r'(?<=\\s)\\w+', article_text)\n","        # print(len(sentences))\n","        # Hint: You can use the following code to get the number of paragraphs in the article\n","        # paragraphs = re.findall(r'(?<=\\n\\s)\\w+', article_text)\n","        # print(len(paragraphs))\n","        # Hint: You can use the following code to get the number of sentences in the article\n","        # sentences = re.findall(r'(?<=\\s)\\w+', article_text)\n","        # print(len(sentences))\n","        # Hint: You can use the following code to get the number of paragraphs in the article\n","        # paragraphs = re.findall(r'(?<=\\n\\s)\\w+', article_text)\n","        # print(len(paragraphs))\n","        # Hint: You can use the following code to get the number of sentences in the article\n","        # sentences = re.findall(r'(?<=\\s)\\w+', article_text)\n","        # print(len(sentences))\n","        # Hint: You can use the following code to get the number of paragraphs in the article\n","        # paragraphs = re.findall(r'(?<=\\n\\s)\\w+', article_text)\n","        # print(len(paragraphs))\n","        # Hint: You can use the following code to get the number of sentences in the article\n","        # sentences = re.findall(r'(?<=\\s)\\w+', article_text)\n","        # print(len(sentences))\n","        # Hint: You can use the following code to get the number of paragraphs in the article\n","        # paragraphs = re.findall(r'(?<=\\n\\s)\\w+', article_text)\n","        # print(len(paragraphs))\n","        # Hint: You can use the following code to get the number of sentences in the article\n","        # sentences = re.findall(r'(?<=\\s)\\w+', article_text)\n","        # print(len(sentences))\n","        # Hint: You can use the following code to get the number of paragraphs in the article\n","        # paragraphs = re.findall(r'(?<=\\n\\s)\\w+', article_text)\n","        # print(len(paragraphs))\n","        # Hint: You can use the following code to get the number of sentences in the article\n","        # sentences = re.findall(r'(?<=\\s)\\w+', article_text)\n","        # print(len(sentences))\n","        # Hint: You can use the following code to get the number of paragraphs in the article\n","        # paragraphs = re.findall(r'(?<=\\n\\s)\\w+', article_text)\n","        # print(len(paragraphs))\n","        # Hint: You can use the following code to get the number of sentences in the article\n","        # sentences = re.findall(r'(?<=\\s)\\w+', article_text)\n","        # print(len(sentences))\n","        # Hint: You can use the following code to get the number of paragraphs in the article\n","        # paragraphs = re.findall(r'(?<=\\n\\s)\\w+', article_text)\n","        # print(len(paragraphs))\n","        # Hint: You can use the following code to get the number of sentences in the article\n","        # sentences = re.findall(r'(?<=\\s)\\w+', article_text)\n","        # print(len(sentences))\n","        # Hint: You can use the following code to get the number of paragraphs in the article\n","        # paragraphs = re.findall(r'(?<=\\n\\s)\\w+', article_text)\n","        # print(len(paragraphs))\n","        # Hint: You can use the following code to get the number of sentences in the article\n","        # sentences = re.findall(r'(?<=\\s)\\w+', article_text)\n","        # print(len(sentences))\n","        # Hint: You can use the following code to get the number of paragraphs in the article\n","        # paragraphs = re.findall(r'(?<=\\n\\s)\\w+', article_text)\n","        # print(len(paragraphs))\n","        # Hint: You can use the following code to get the number of sentences in the article\n","        # sentences = re.findall(r'(?<=\\s)\\w+', article_text)\n","        # print(len(sentences))\n","        # Hint: You can use the following code to get the number of paragraphs in the article\n","        # paragraphs = re.findall(r'(?<=\\n\\s)\\w+', article_text)\n","        # print(len(paragraphs))\n","        # Hint: You can use the following code to get the number of sentences in the article\n","        # sentences = re.findall(r'(?<=\\s)\\w+', article_text)\n","        # print(len(sentences))\n","        # Hint: You can use the following code to get the number of paragraphs in the article\n","        # paragraphs = re.findall(r'(?<=\\n\\s)\\w+', article_text)\n","        # print(len(paragraphs))\n","        # Hint: You can use the following code to get the number of sentences in the article\n","        # sentences = re.findall(r'(?<=\\s)\\w+', article_text)\n","        # print(len(sentences))\n","        # Hint: You can use the following code to get the number of paragraphs in the article\n","        # paragraphs = re.findall(r'(?<=\\n\\s)\\w+', article_text)\n","        # print(len(paragraphs))\n","        # Hint: You can use the following code to get the number of sentences in the article\n","        # sentences = re.findall(r'(?<=\\s)\\w+', article_text)\n","        # print(len(sentences))\n","        # Hint: You can use the following code to get the number of paragraphs in the article\n","        # paragraphs = re.findall(r'(?<=\\n\\s)\\w+', article_text)\n","        # print(len(paragraphs))\n","        # Hint: You can use the following code to get the number of sentences in the article\n","        # sentences = re.findall(r'(?<=\\s)\\w+', article_text)\n","        # print(len(sentences))\n","        # Hint: You can use the following code to get the number of paragraphs in the article\n","        # paragraphs = re.findall(r'(?<=\\n\\s)\\w+', article_text)\n","        # print(len(paragraphs))\n","        # Hint: You can use the following code to get the number of sentences in the article\n","        # sentences = re.findall(r'(?<=\\s)\\w+', article_text)\n","        # print(len(sentences))\n","        # Hint: You can use the following code to get the number of paragraphs in the article\n","        # paragraphs = re.findall(r'(?<=\\n\\s)\\w+', article_text)\n","        # print(len(paragraphs))\n","        # Hint: You can use the following code to get the number of sentences in the article\n","        # sentences = re.findall(r'(?<=\\s)\\w+', article_text)\n","        # print(len(sentences))\n","        # Hint: You can use the following code to get the number of paragraphs in the article\n","        #\n","Extracted Information:\n","Problem    : None\n","Solution   : None\n","Limitation : None\n"]}]},{"cell_type":"code","source":[],"metadata":{"id":"Xsn2gBvoiUW7"},"execution_count":null,"outputs":[]}]}